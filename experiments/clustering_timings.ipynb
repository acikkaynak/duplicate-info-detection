{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import modin.pandas as pd\n",
    "#from pandarallel import pandarallel\n",
    "#import swifter\n",
    "#from swifter import set_defaults\n",
    "#import ray\n",
    "\n",
    "#swifter.register_modin()       #USE IT ONLY W/ MODIN IF YOU IMPORT SWIFTER BEFORE MODIN\n",
    "\n",
    "distance_calcs = []\n",
    "awesome_cossim_calc = []\n",
    "toarray_calc = []\n",
    "dim_reduction =[]\n",
    "dbscan_clustering = []\n",
    "vectorizing = [] \n",
    "attribution = []    \n",
    "\n",
    "def load_data(data_path: str, preprocess_save_path: str = \"data/df_preprocessed.pkl\") -> pd.DataFrame:\n",
    "    if data_path.endswith(\".pkl\"):\n",
    "        df = pd.read_pickle(data_path)\n",
    "        print(f\"Loaded processed data from {data_path}, with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    elif data_path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(data_path)\n",
    "        df = df.fillna(\"\")\n",
    "        print(f\"Loaded unprocessed data from {data_path}, with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        #df = run_preprocess(df)\n",
    "        #print(f\"Processed the data, ended up with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        #pd.to_pickle(df, preprocess_save_path)\n",
    "    return df\n",
    "\n",
    "def merge_address_columns(\n",
    "        df: pd.DataFrame,\n",
    "        new_column_name: str = \"merged_address\"\n",
    "    ) -> pd.DataFrame:\n",
    "    df[new_column_name] = df['Bina Adı'] + \" \" + df['Dış Kapı/ Blok/Apartman No'] \\\n",
    "        + \" \" + df[\"Bulvar/Cadde/Sokak/Yol/Yanyol\"] + \" \" + df[\"new_adres\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(\n",
    "    \n",
    "        df: pd.DataFrame,\n",
    "        name_duplicate_max_distance_threshold: float,\n",
    "        address_duplicate_max_distance_threshold: float,\n",
    "        tfidf_ngram_range: tuple,\n",
    "        tfidf_min_df: int,\n",
    "        tfidf_use_char_ngrams: bool,\n",
    "\n",
    ") -> pd.DataFrame:\n",
    "      \n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import hdbscan\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    #from sklearn.metrics.pairwise import pairwise_kernel\n",
    "    import numpy as np\n",
    "    #from numpy import ones_like\n",
    "    from timeit import default_timer as timer\n",
    "    from sparse_dot_topn import awesome_cossim_topn\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from scipy import sparse\n",
    "\n",
    "\n",
    "    df.loc[:, \"clustering_index\"] = -1\n",
    "        \n",
    "    def cluster_group(group_df):\n",
    "\n",
    "        def cluster_by_column(\n",
    "        df: pd.DataFrame,\n",
    "        key_column_name: str,\n",
    "        duplicate_max_distance_threshold: float,\n",
    "        tfidf_ngram_range: tuple,\n",
    "        tfidf_min_df: int,\n",
    "        tfidf_use_char_ngrams: bool,\n",
    "        df_mask,\n",
    "    ) -> pd.DataFrame:\n",
    "            \n",
    "            #np.seterr(divide='ignore', invalid='ignore')         #USE IT ONLY W/ SVD\n",
    "            \n",
    "            # index the rows that will be clustered\n",
    "            df.loc[df_mask, \"clustering_index\"] = list(range(df.loc[df_mask].shape[0]))\n",
    "\n",
    "            # derive names for cluster information columns\n",
    "            cluster_column_name = f\"{key_column_name}-cluster\"\n",
    "            duplicate_info_column_name = f\"{key_column_name}-duplicate\"\n",
    "            similarity_column_name = f\"{key_column_name}-duplicate-similarity\"\n",
    "            duplicate_original_column_name = f\"{key_column_name}-duplicate-original-id\"\n",
    "\n",
    "            analyzer = \"char_wb\" if tfidf_use_char_ngrams else \"word\"\n",
    "            vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=tfidf_ngram_range, min_df=tfidf_min_df)\n",
    "            vectorizer_start = timer()\n",
    "            vectors = vectorizer.fit_transform(df.loc[df_mask, key_column_name])\n",
    "            vectorizer_end = timer()\n",
    "            vectorizing.append(vectorizer_end-vectorizer_start)\n",
    "\n",
    "            #reducer = TruncatedSVD(10)\n",
    "            #dim_reduction_start = timer()\n",
    "            #vectors = reducer.fit_transform(vectors)\n",
    "            #vectors = sparse.csr_matrix(vectors)\n",
    "            #dim_reduction_end = timer()\n",
    "            #dim_reduction.append(dim_reduction_end-dim_reduction_start)\n",
    "        \n",
    "            awesome_cossim_start = timer()\n",
    "            #vectors = vectors.astype(np.float32)\n",
    "            awesome_cossim = awesome_cossim_topn(vectors, vectors.T, 10, 0.8)\n",
    "            #awesome_distance = 1 - awesome_cossim\n",
    "            awesome_cossim_end = timer()\n",
    "            awesome_cossim_calc.append(awesome_cossim_end-awesome_cossim_start)\n",
    "\n",
    "            #if awesome_cossim > 1: \n",
    "            #    print(awesome_cossim)\n",
    "                \n",
    "            toarray_start = timer()\n",
    "            awesome_cossim = awesome_cossim.toarray()\n",
    "            toarray_end = timer()\n",
    "            toarray_calc.append(toarray_end-toarray_start)\n",
    "\n",
    "            # compute pairwise cosine distances between all rows\n",
    "            start = timer()\n",
    "            #distance_matrix = pairwise_distances(vectors, vectors, metric=\"cosine\")\n",
    "            distance_matrix = np.abs(1 - awesome_cossim)\n",
    "            end = timer()\n",
    "            distance_calcs.append(end-start)\n",
    "            \n",
    "            # run the DBSCAN clustering algorithm using the pairwise distances\n",
    "            dbscan_start = timer()\n",
    "            '''\n",
    "            dbscan = DBSCAN(eps=duplicate_max_distance_threshold, min_samples=2, metric=\"precomputed\") \\\n",
    "                .fit(distance_matrix)\n",
    "            '''\n",
    "            dbscan = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=2, cluster_selection_epsilon=0.85, cluster_selection_method='leaf').fit(distance_matrix)\n",
    "\n",
    "            dbscan_end = timer()\n",
    "            dbscan_clustering.append(dbscan_end-dbscan_start)\n",
    "           \n",
    "            # annotate each row with the id of the cluster it belongs to\n",
    "            df.loc[df_mask, cluster_column_name] = dbscan.labels_\n",
    "        \n",
    "            attribution_start = timer()\n",
    "            # process each cluster for marking reference points and similarity scores to the reference point\n",
    "            for cluster in np.unique(dbscan.labels_):\n",
    "                if cluster == \"-1\":\n",
    "                    # -1 is the no-cluster cluster label, no need to do anything else\n",
    "                    continue\n",
    "\n",
    "                # create a mask for the rows in the cluster\n",
    "                cluster_mask = df_mask & (df[cluster_column_name] == cluster)\n",
    "\n",
    "                # the first entry in the cluster is picked as the \"original\"\n",
    "                original_row_mask = df.index[cluster_mask][0]\n",
    "                duplicate_row_mask = df.index[cluster_mask][1:]\n",
    "                original_row = df.loc[original_row_mask]\n",
    "\n",
    "                # fetch the similarities of each row to the original row\n",
    "                #original_row_similarities = 1.0 - distance_matrix[original_row[\"clustering_index\"], :]\n",
    "                original_row_similarities = awesome_cossim[original_row[\"clustering_index\"], :]\n",
    "\n",
    "\n",
    "                # mark the original row with \"O\"\n",
    "                df.loc[original_row_mask, duplicate_info_column_name] = \"O\"\n",
    "                # mark the other rows with \"D\"\n",
    "                df.loc[duplicate_row_mask, duplicate_info_column_name] = \"D\"\n",
    "                # mark every row with the original row's id\n",
    "                df.loc[cluster_mask, duplicate_original_column_name] = original_row[\"id\"]\n",
    "                # mark every row with the similarity score to the original row\n",
    "                df.loc[cluster_mask, similarity_column_name] = original_row_similarities[df.loc[cluster_mask, \"clustering_index\"]]\n",
    "            attribution_end = timer()\n",
    "            attribution.append(attribution_end-attribution_start)\n",
    "\n",
    "            return df \n",
    "\n",
    "        try:\n",
    "\n",
    "            # only cluster names that are defined\n",
    "            name_defined_mask = (group_df[\"Ad-Soyad\"] != \"\")\n",
    "            group_df = cluster_by_column(\n",
    "                df=group_df, \n",
    "                key_column_name=\"Ad-Soyad\", \n",
    "                duplicate_max_distance_threshold=name_duplicate_max_distance_threshold, \n",
    "                tfidf_ngram_range=tfidf_ngram_range, \n",
    "                tfidf_min_df=tfidf_min_df, \n",
    "                tfidf_use_char_ngrams=tfidf_use_char_ngrams, \n",
    "                df_mask = name_defined_mask)\n",
    "            \n",
    "            # if name is not defined, assign -1 to cluster (means no cluster)\n",
    "            group_df.loc[group_df[\"Ad-Soyad\"] == \"\", \"Ad-Soyad-cluster\"] = -1   \n",
    "                 \n",
    "        except ValueError as e:\n",
    "            # in case of any errors when clustering names, assign -1 to every row (means no cluster)\n",
    "            group_df[\"Ad-Soyad-cluster\"] = -1\n",
    "        name_clusters = group_df[\"Ad-Soyad-cluster\"].unique()\n",
    "        for name_cluster in name_clusters:\n",
    "            cluster_df_mask = (group_df[\"Ad-Soyad-cluster\"] == name_cluster)\n",
    "            if name_cluster == -1:\n",
    "                # If name is not in any cluster, no need to cluster addresses\n",
    "                group_df.loc[cluster_df_mask, 'merged_address-cluster'] = -1\n",
    "                continue\n",
    "            try:\n",
    "                group_df = cluster_by_column(\n",
    "                    df=group_df, \n",
    "                    key_column_name=\"merged_address\", \n",
    "                    duplicate_max_distance_threshold=address_duplicate_max_distance_threshold, \n",
    "                    tfidf_ngram_range=tfidf_ngram_range, \n",
    "                    tfidf_min_df=tfidf_min_df, \n",
    "                    tfidf_use_char_ngrams=tfidf_use_char_ngrams, \n",
    "                    df_mask=cluster_df_mask)\n",
    "            except ValueError as e:\n",
    "                # in case of any errors when clustering addresses, assign -1 to every row (means no cluster)\n",
    "                group_df.loc[cluster_df_mask, 'merged_address-cluster'] = -1\n",
    "        return group_df\n",
    "\n",
    " \n",
    "\n",
    "    #grouping_start = timer()\n",
    "    df = df.groupby([\"İl\", \"İlçe\", \"Mahalle\"], group_keys=False)\n",
    "    #grouping_end = timer()\n",
    "    #print(\"TIME ELAPSED IN GROUPING: \", grouping_end-grouping_start)\n",
    "\n",
    "    apply_method_start = timer()\n",
    "    df = df.apply(cluster_group)                                                          #DEFAULT SINGLE THREADED PANDAS APPLY OR MODIN\n",
    "    #df.swifter.groupby([\"İl\", \"İlçe\", \"Mahalle\"], group_keys=False).apply(cluster_group)  #SWIFTER \n",
    "    #df.groupby([\"İl\", \"İlçe\", \"Mahalle\"], group_keys=False).parallel_apply(cluster_group) #PANDARALLEL\n",
    "    apply_method_end = timer()\n",
    "    print(\"TIME ELAPSED IN APPLY cluster_group METHOD:\", apply_method_end-apply_method_start)\n",
    "\n",
    "    df.drop(\"clustering_index\", axis=1, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "        data_path: str = \"data\\preprocessed_data.csv\",\n",
    "        name_duplicate_max_distance_threshold: float = 0.2,\n",
    "        address_duplicate_max_distance_threshold: float = 0.6,\n",
    "        tfidf_ngram_min: int = 2,\n",
    "        tfidf_ngram_max: int = 4,\n",
    "        tfidf_use_char_ngrams: bool = True,\n",
    "        output_data_path: str = \"data\\clustered_v_1_4.csv\",\n",
    "        save_clustered_csv: bool = False,\n",
    "        tfidf_min_df=1\n",
    "    ):\n",
    "\n",
    "    '''\n",
    "    #Swifter config, affects Dask\n",
    "    set_defaults(\n",
    "        npartitions=None,\n",
    "        dask_threshold=1,\n",
    "        scheduler=\"processes\",\n",
    "        progress_bar=True,\n",
    "        progress_bar_desc=None,\n",
    "        allow_dask_on_strings=False,\n",
    "        force_parallel=False,\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    df_main = load_data(data_path)\n",
    "    df_main = merge_address_columns(df_main)\n",
    "    \n",
    "    #pandarallel.initialize(nb_workers=4, progress_bar=True)\n",
    "\n",
    "    df_main = cluster_data(\n",
    "        df=df_main, \n",
    "        name_duplicate_max_distance_threshold=name_duplicate_max_distance_threshold, \n",
    "        address_duplicate_max_distance_threshold=address_duplicate_max_distance_threshold, \n",
    "        tfidf_ngram_range=(tfidf_ngram_min, tfidf_ngram_max),\n",
    "        tfidf_use_char_ngrams=tfidf_use_char_ngrams,\n",
    "        tfidf_min_df=tfidf_min_df,\n",
    "    )\n",
    "\n",
    "    print(\"TIME ELAPSED IN VECTORIZER: \", sum(vectorizing))\n",
    "    vectorizing.clear()\n",
    "    #print(\"TIME ELAPSED IN DIM. REDUCTION: \", sum(dim_reduction))\n",
    "    #dim_reduction.clear()\n",
    "    print(\"TIME ELAPSED IN AWESOME COSSIM CALCULATIONS:\", sum(awesome_cossim_calc))\n",
    "    awesome_cossim_calc.clear()\n",
    "    print(\"TIME ELAPSED IN COSINE DISTANCE CALCULATION: \", sum(distance_calcs))\n",
    "    distance_calcs.clear()\n",
    "    print(\"TIME ELAPSED IN .toarray: \", sum(toarray_calc))\n",
    "    toarray_calc.clear()\n",
    "    print(\"TIME ELAPSED IN DBSCAN/HDBSCAN CLUSTERING: \", sum(dbscan_clustering)) \n",
    "    dbscan_clustering.clear()\n",
    "    print(\"TIME ELAPSED IN PRINTING ATTRIBUTIONS: \", sum(attribution))\n",
    "    attribution.clear()\n",
    "    \n",
    "    if save_clustered_csv:\n",
    "        df_main.to_csv(output_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b5bb7282a6ddf7cc4bcae726025236b533ec09a41c1db2a7543ade93befbff8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
