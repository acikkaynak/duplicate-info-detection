{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import numpy as np\n",
    "from preprocess_funcs import run_preprocess\n",
    "from timeit import default_timer as timer\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "distance_calcs = []\n",
    "awesome_cossim_calc = []\n",
    "dim_reduction =[]\n",
    "dbscan_clustering = []\n",
    "vectorizing = [] \n",
    "attribution = []\n",
    "\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def load_data(data_path: str, preprocess_save_path: str = \"data/df_preprocessed.pkl\") -> pd.DataFrame:\n",
    "    if data_path.endswith(\".pkl\"):\n",
    "        df = pd.read_pickle(data_path)\n",
    "        print(f\"Loaded processed data from {data_path}, with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    elif data_path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(data_path)\n",
    "        df = df.fillna(\"\")\n",
    "        print(f\"Loaded unprocessed data from {data_path}, with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        df = run_preprocess(df)\n",
    "        print(f\"Processed the data, ended up with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        pd.to_pickle(df, preprocess_save_path)\n",
    "    return df\n",
    "\n",
    "def merge_address_columns(\n",
    "        df: pd.DataFrame,\n",
    "        new_column_name: str = \"merged_address\"\n",
    "    ) -> pd.DataFrame:\n",
    "    df[new_column_name] = df['Bina Adı'] + \" \" + df['Dış Kapı/ Blok/Apartman No'] \\\n",
    "        + \" \" + df[\"Bulvar/Cadde/Sokak/Yol/Yanyol\"] + \" \" + df[\"new_adres\"]\n",
    "    return df\n",
    "def cluster_by_column(\n",
    "        df: pd.DataFrame,\n",
    "        key_column_name: str,\n",
    "        duplicate_max_distance_threshold: float,\n",
    "        tfidf_ngram_range: tuple,\n",
    "        tfidf_min_df: int,\n",
    "        tfidf_use_char_ngrams: bool,\n",
    "        df_mask,\n",
    ") -> pd.DataFrame:\n",
    "    # index the rows that will be clustered\n",
    "    df.loc[df_mask, \"clustering_index\"] = list(range(df.loc[df_mask].shape[0]))\n",
    "\n",
    "    # derive names for cluster information columns\n",
    "    cluster_column_name = f\"{key_column_name}-cluster\"\n",
    "    duplicate_info_column_name = f\"{key_column_name}-duplicate\"\n",
    "    similarity_column_name = f\"{key_column_name}-duplicate-similarity\"\n",
    "    duplicate_original_column_name = f\"{key_column_name}-duplicate-original-id\"\n",
    "\n",
    "    analyzer = \"char_wb\" if tfidf_use_char_ngrams else \"word\"\n",
    "    vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=tfidf_ngram_range, min_df=tfidf_min_df)\n",
    "    vectorizer_start = timer()\n",
    "    vectors = vectorizer.fit_transform(df.loc[df_mask, key_column_name])\n",
    "    vectorizer_end = timer()\n",
    "    vectorizing.append(vectorizer_end-vectorizer_start)\n",
    "\n",
    "    \n",
    "    #reducer = TruncatedSVD(1000)\n",
    "    #dim_reduction_start = timer()\n",
    "    #reduced_vectors = reducer.fit_transform(vectors)\n",
    "    #dim_reduction_end = timer()\n",
    "    #dim_reduction.append(dim_reduction_end-dim_reduction_start)\n",
    "    #reduced_vectors = sparse.csr_matrix(reduced_vectors)\n",
    "    \n",
    "    \n",
    "    #awesome_cossim_start = timer()\n",
    "    #awesome_cossim = awesome_cossim_topn(reduced_vectors, reduced_vectors.T, 10, 0.4).toarray()\n",
    "    #awesome_cossim_end = timer()\n",
    "    #awesome_cossim_calc.append(awesome_cossim_end-awesome_cossim_start)\n",
    "\n",
    "    \n",
    "    # compute pairwise cosine distances between all rows\n",
    "    start = timer()\n",
    "    distance_matrix = pairwise_distances(vectors, vectors, metric=\"cosine\")\n",
    "    end = timer()\n",
    "    distance_calcs.append(end-start)\n",
    "    \n",
    "    \n",
    "    # run the DBSCAN clustering algorithm using the pairwise distances\n",
    "    dbscan_start = timer()\n",
    "    dbscan = DBSCAN(eps=duplicate_max_distance_threshold, min_samples=2, metric=\"precomputed\") \\\n",
    "        .fit(distance_matrix)\n",
    "    dbscan_end = timer()\n",
    "    dbscan_clustering.append(dbscan_end-dbscan_start)\n",
    "\n",
    "    \n",
    "    # annotate each row with the id of the cluster it belongs to\n",
    "    df.loc[df_mask, cluster_column_name] = dbscan.labels_\n",
    "   \n",
    "    attribution_start = timer()\n",
    "    # process each cluster for marking reference points and similarity scores to the reference point\n",
    "    for cluster in np.unique(dbscan.labels_):\n",
    "        if cluster == \"-1\":\n",
    "            # -1 is the no-cluster cluster label, no need to do anything else\n",
    "            continue\n",
    "\n",
    "        # create a mask for the rows in the cluster\n",
    "        cluster_mask = df_mask & (df[cluster_column_name] == cluster)\n",
    "\n",
    "        # the first entry in the cluster is picked as the \"original\"\n",
    "        original_row_mask = df.index[cluster_mask][0]\n",
    "        duplicate_row_mask = df.index[cluster_mask][1:]\n",
    "        original_row = df.loc[original_row_mask]\n",
    "\n",
    "        # fetch the similarities of each row to the original row\n",
    "        original_row_similarities = 1.0 - distance_matrix[original_row[\"clustering_index\"], :]\n",
    "\n",
    "        # mark the original row with \"O\"\n",
    "        df.loc[original_row_mask, duplicate_info_column_name] = \"O\"\n",
    "        # mark the other rows with \"D\"\n",
    "        df.loc[duplicate_row_mask, duplicate_info_column_name] = \"D\"\n",
    "        # mark every row with the original row's id\n",
    "        df.loc[cluster_mask, duplicate_original_column_name] = original_row[\"id\"]\n",
    "        # mark every row with the similarity score to the original row\n",
    "        df.loc[cluster_mask, similarity_column_name] = original_row_similarities[df.loc[cluster_mask, \"clustering_index\"]]\n",
    "    attribution_end = timer()\n",
    "    attribution.append(attribution_end-attribution_start)\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(\n",
    "        df: pd.DataFrame,\n",
    "        name_duplicate_max_distance_threshold: float,\n",
    "        address_duplicate_max_distance_threshold: float,\n",
    "        tfidf_ngram_range: tuple,\n",
    "        tfidf_min_df: int,\n",
    "        tfidf_use_char_ngrams: bool,\n",
    ") -> pd.DataFrame:\n",
    "    df.loc[:, \"clustering_index\"] = -1\n",
    "    def cluster_group(group_df):\n",
    "        try:\n",
    "            # only cluster names that are defined\n",
    "            name_defined_mask = (group_df[\"Ad-Soyad\"] != \"\")\n",
    "            group_df = cluster_by_column(\n",
    "                df=group_df, \n",
    "                key_column_name=\"Ad-Soyad\", \n",
    "                duplicate_max_distance_threshold=name_duplicate_max_distance_threshold, \n",
    "                tfidf_ngram_range=tfidf_ngram_range, \n",
    "                tfidf_min_df=tfidf_min_df, \n",
    "                tfidf_use_char_ngrams=tfidf_use_char_ngrams, \n",
    "                df_mask = name_defined_mask)\n",
    "            # if name is not defined, assign -1 to cluster (means no cluster)\n",
    "            group_df.loc[group_df[\"Ad-Soyad\"] == \"\", \"Ad-Soyad-cluster\"] = -1\n",
    "        except ValueError as e:\n",
    "            # in case of any errors when clustering names, assign -1 to every row (means no cluster)\n",
    "            group_df[\"Ad-Soyad-cluster\"] = -1\n",
    "        name_clusters = group_df[\"Ad-Soyad-cluster\"].unique()\n",
    "        for name_cluster in name_clusters:\n",
    "            cluster_df_mask = (group_df[\"Ad-Soyad-cluster\"] == name_cluster)\n",
    "            if name_cluster == -1:\n",
    "                # If name is not in any cluster, no need to cluster addresses\n",
    "                group_df.loc[cluster_df_mask, 'merged_address-cluster'] = -1\n",
    "                continue\n",
    "            try:\n",
    "                group_df = cluster_by_column(\n",
    "                    df=group_df, \n",
    "                    key_column_name=\"merged_address\", \n",
    "                    duplicate_max_distance_threshold=address_duplicate_max_distance_threshold, \n",
    "                    tfidf_ngram_range=tfidf_ngram_range, \n",
    "                    tfidf_min_df=tfidf_min_df, \n",
    "                    tfidf_use_char_ngrams=tfidf_use_char_ngrams, \n",
    "                    df_mask=cluster_df_mask)\n",
    "            except ValueError as e:\n",
    "                # in case of any errors when clustering addresses, assign -1 to every row (means no cluster)\n",
    "                group_df.loc[cluster_df_mask, 'merged_address-cluster'] = -1\n",
    "        return group_df\n",
    "    \n",
    "    grouping_start = timer()\n",
    "    df = df.groupby([\"İl\", \"İlçe\", \"Mahalle\"], group_keys=False)\n",
    "    grouping_end = timer()\n",
    "    print(\"TIME ELAPSED IN GROUPING: \", grouping_end-grouping_start)\n",
    "\n",
    "    apply_method_start = timer()\n",
    "    df = df.apply(cluster_group)\n",
    "    apply_method_end = timer()\n",
    "    print(\"TIME ELAPSED IN APPLY cluster_group METHOD:\", apply_method_end-apply_method_start)\n",
    "\n",
    "    df.drop(\"clustering_index\", axis=1, inplace=True)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "        data_path: str = \"data/merged_v1_4.csv\",\n",
    "        name_duplicate_max_distance_threshold: float = 0.2,\n",
    "        address_duplicate_max_distance_threshold: float = 0.3,\n",
    "        tfidf_ngram_min: int = 3,\n",
    "        tfidf_ngram_max: int = 4,\n",
    "        tfidf_use_char_ngrams: bool = True,\n",
    "        output_data_path: str = \"data/clustered_v_1_4.csv\",\n",
    "        save_clustered_csv: bool = False,\n",
    "    ):\n",
    "    df_main = load_data(data_path)\n",
    "    df_main = merge_address_columns(df_main)\n",
    "    df_main = cluster_data(\n",
    "        df=df_main, \n",
    "        name_duplicate_max_distance_threshold=name_duplicate_max_distance_threshold, \n",
    "        address_duplicate_max_distance_threshold=address_duplicate_max_distance_threshold, \n",
    "        tfidf_ngram_range=(tfidf_ngram_min, tfidf_ngram_max),\n",
    "        tfidf_use_char_ngrams=tfidf_use_char_ngrams,\n",
    "        tfidf_min_df=1,\n",
    "    )\n",
    "\n",
    "    print(\"TIME ELAPSED IN VECTORIZER: \", sum(vectorizing))\n",
    "    vectorizing.clear()\n",
    "    #print(\"TIME ELAPSED IN DIM. REDUCTION: \", sum(dim_reduction))\n",
    "    dim_reduction.clear()\n",
    "    print(\"TIME ELAPSED IN COSINE DISTANCE CALCULATION: \", sum(distance_calcs))\n",
    "    distance_calcs.clear()\n",
    "    #print(\"TIME ELAPSED IN AWESOME CALCULATIONS:\", sum(awesome_cossim_calc))\n",
    "    awesome_cossim_calc.clear()\n",
    "    print(\"TIME ELAPSED IN DBSCAN CLUSTERING: \", sum(dbscan_clustering)) \n",
    "    dbscan_clustering.clear()\n",
    "    print(\"TIME ELAPSED IN PRINTING ATTRIBUTIONS: \", sum(attribution))\n",
    "    attribution.clear()\n",
    "    \n",
    "    if save_clustered_csv:\n",
    "        df_main.to_csv(output_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c932e7a6bb27c55ab3a1bdc04e550bcb586768f33d1ef512444ebde2d57752b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
